{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成的噪声数据替换点： 21276696\n",
      "替换的噪声符号：\n",
      "〖 \n",
      "〗 — ｉ ｏ ｇ ｅ ｎ ＋ ｄ · ｓ ｃ ｂ ａ ｒ ｊ \u001A ｕ ～ ｔ ｈ － ｌ ─ ＜ ＞ ｐ ３ ｋ Ｖ ＠ ○ １ ９ 〓 ① ② ③ Ｈ Ｃ Ａ Ｔ ｙ ｆ ④ ⑤ Ｇ Ｓ ⑥ ⑧ \n",
      "● ４ ｍ ⑦ ┦    ∶ Ｐ ＂ Ｋ  Ｍ ６ ⑨ ×  ５ ※   ２ ８ ０ Ｂ ｗ Ｒ Ｌ    ｘ ｚ Ｗ         ［ ］  Ｎ Ｆ ｖ ＝ \n",
      "Ｅ Ｄ Ｙ Ｊ     Γ τ        ⑹   ⒅  ⒑     ば ／ ￣  Ｏ   α  ィ      Φ χ   ソ   ⑼ \n",
      "￡   ο   π  С     〈   〕 ７ δ   ⒌  ǖ ァ   Υ ℃     Ｑ  ＼ é Ｉ  ⒃ ∩    セ υ ぶ    \n",
      "        ビ    ㄍ  ゲ        Τ ∽ ぷ     ㄊ  Ω             ⒍   ぃ   \n",
      "ジ  Ψ  ⑸         ∪  ⑾    σ ⑷  て  ┯   Ｘ Ｚ Α   ┢ ㈡   Ρ  〉  ┳ チ   ɑ シ ┕  ㄅ  \n",
      "┒ б  ┰        Щ γ     ド ≌   ζ  ち  ∑    ξ  ∝  ┨   ń      ┬      フ  \n",
      "   ρ ＃ "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "def DFS_file_search(dict_name):\n",
    "    # list.pop() list.append()这两个方法就可以实现栈维护功能\n",
    "    stack = []\n",
    "    result_txt = []\n",
    "    stack.append(dict_name)\n",
    "    while len(stack) != 0:  # 栈空代表所有目录均已完成访问\n",
    "        temp_name = stack.pop()\n",
    "        try:\n",
    "            temp_name2 = os.listdir(temp_name)  # list [\"\",\"\",...]\n",
    "            for eve in temp_name2:\n",
    "                stack.append(temp_name + \"\\\\\" + eve)  # 维持绝对路径的表达\n",
    "        except NotADirectoryError:\n",
    "            result_txt.append(temp_name)\n",
    "    return result_txt\n",
    "path_list = DFS_file_search(r\".\\data\")\n",
    "# path_list 为包含所有小说文件的路径列表\n",
    "corpus = []\n",
    "for path in path_list:\n",
    "    with open(path, \"r\", encoding=\"ANSI\") as file:\n",
    "        text = [line.strip(\"\\n\").replace(\"\\u3000\", \"\").replace(\"\\t\", \"\") for line in file][3:]\n",
    "        corpus += text\n",
    "# corpus 存储语料库，其中以每一个自然段为一个分割\n",
    "regex_str = \".*?([^\\u4E00-\\u9FA5]).*?\"\n",
    "english = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:：;「<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'\n",
    "symbol = []\n",
    "for j in range(len(corpus)):\n",
    "    corpus[j] = re.sub(english, \"\", corpus[j])\n",
    "    symbol += re.findall(regex_str, corpus[j])\n",
    "count_ = Counter(symbol)\n",
    "count_symbol = count_.most_common()\n",
    "noise_symbol = []\n",
    "for eve_tuple in count_symbol:\n",
    "    if eve_tuple[1] < 200:\n",
    "        noise_symbol.append(eve_tuple[0])\n",
    "noise_number = 0\n",
    "for line in corpus:\n",
    "    for noise in noise_symbol:\n",
    "        line.replace(noise, \"\")\n",
    "        noise_number += 1\n",
    "print(\"完成的噪声数据替换点：\", noise_number)\n",
    "print(\"替换的噪声符号：\")\n",
    "for i in range(len(noise_symbol)):\n",
    "    print(noise_symbol[i], end=\" \")\n",
    "    if i % 50 == 0:\n",
    "        print()\n",
    "with open(\"预处理后的文本.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in corpus:\n",
    "        if len(line) > 1:\n",
    "            print(line, file=f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter\n",
    "import math\n",
    "with open(\"预处理后的文本.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [eve.strip(\"\\n\") for eve in f]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\pc\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.620 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词库总词数： 4314424   不同词的个数： 173004\n",
      "出现频率前10的1-gram词语： [('的', 115604), ('了', 104527), ('他', 64712), ('是', 64457), ('道', 58623), ('我', 57483), ('你', 56679), ('在', 43691), ('也', 32606), ('这', 32199)]\n",
      "entropy_1gram: 12.1667882280937\n"
     ]
    }
   ],
   "source": [
    "# 1-gram\n",
    "token = []\n",
    "for para in corpus:\n",
    "    token += jieba.lcut(para)\n",
    "token_num = len(token)\n",
    "ct = Counter(token)\n",
    "vocab1 = ct.most_common()\n",
    "entropy_1gram = sum([-(eve[1]/token_num)*math.log((eve[1]/token_num),2) for eve in vocab1])\n",
    "print(\"词库总词数：\", token_num, \" \", \"不同词的个数：\", len(vocab1))\n",
    "print(\"出现频率前10的1-gram词语：\", vocab1[:10])\n",
    "print(\"entropy_1gram:\", entropy_1gram)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
